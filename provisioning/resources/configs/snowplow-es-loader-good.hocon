# Copyright (c) 2014-2021 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

{
  "input": {
    # Sources currently supported are:
    # "kinesis" for reading records from a Kinesis stream
    # "stdin" for reading unencoded tab-separated events from stdin
    # If set to "stdin", JSON documents will not be sent to Elasticsearch
    # but will be written to stdout.
    # "nsq" for reading unencoded tab-separated events from NSQ
    "type": "nsq"

    # Stream name for incoming data
    "streamName": "EnrichedEvents"

    # Channel name for NSQ source
    # If more than one application reading from the same NSQ topic at the same time,
    # all of them must have unique channel name for getting all the data from the same topic
    "channelName": "ESLoaderChannelGood"

    # Host name for nsqlookupd
    "nsqlookupdHost": "nsqlookupd"

    # HTTP port for nsqd
    "nsqlookupdPort": 4161

    # Events are accumulated in a buffer before being sent to Elasticsearch.
    # The buffer is emptied:
    # - whenever the number of stored records exceeds recordLimit
    # - whenever the combined of stored records exceeds byteLimit
    # - at regular intervals, controlled by timeLimit in milliseconds
    # This value is optional.
    "buffer": {
      "byteLimit": 1000000
      "recordLimit" = 500
      "timeLimit" = 500
    }
  }

  "output": {
    "good": {
      # Good sinks currently supported are:
      # "elasticsearch" for writing good records to Elasticsearch
      # "stdout" for writing good records to stdout
      # Default value "elasticsearch"
      "type": "elasticsearch"

      # Events are indexed using an Elasticsearch Client
      # - endpoint: the cluster endpoint
      # - port (optional, default value 9200): the port the cluster can be accessed on
      #   - for http this is usually 9200
      #   - for transport this is usually 9300
      # - username (optional, remove if not active): http basic auth username
      # - password (optional, remove if not active): http basic auth password
      # - shardDateFormat (optional, remove if not needed): formatting used for sharding good stream, i.e. _yyyy-MM-dd
      # - shardDateField (optional, if not specified derived_tstamp is used): timestamp field for sharding good stream
      # - indexTimeout (optional, default value 60000): the maximum time to wait in milliseconds for a single http transaction when indexing events
      # - maxTimeout (optional, default value 10000): the maximum time to wait in milliseconds between retries after load failures
      # - maxRetries (optional, default value 6): the maximum number of request attempts before giving up
      # - ssl (optional, default value false): if using the http client, whether to use ssl or not
      "client": {
        "endpoint": "elasticsearch"
        "port": 9200
        "maxTimeout" = "10000"
        "maxRetries" = 3
        "ssl" = false
      }

      # When using the AWS ES service
      # - signing: if using the http client and the AWS ES service you can sign your requests
      #    http://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html
      # - region where the AWS ES service is located
      # These values are optional.
      aws {
        "signing" = false
        "region" = "eu-central-1"
      }

      "cluster": {
        # The Elasticsearch index name
        # Default value "good"
        "index": "good"
      }
    }
    "bad" {
      # Bad sinks currently supported are:
      # "kinesis" for writing bad records to Kinesis
      # "stderr" for writing bad records to stderr
      # "nsq" for writing bad records to NSQ
      # "none" for ignoring bad records
      "type": "nsq"

      # Stream name for events which are rejected by Elasticsearch
      "streamName": "BadElasticsearchEvents"

      # Host name for nsqd
      "nsqdHost": "nsqd"
      # HTTP port for nsqd
      "nsqdPort": 4150
    }
  }

  # "ENRICHED_EVENTS" for a stream of successfully enriched events
  # "BAD_ROWS" for a stream of bad events
  # "JSON" for writing plain json
  "purpose": "ENRICHED_EVENTS"
}

