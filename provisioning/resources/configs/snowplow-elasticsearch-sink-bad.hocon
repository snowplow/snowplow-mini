# Default configuration for kinesis-elasticsearch-sink

sink {

  # Sources currently supported are:
  # 'kinesis' for reading records from a Kinesis stream
  # 'stdin' for reading unencoded tab-separated events from stdin
  # If set to "stdin", JSON documents will not be sent to Elasticsearch
  # but will be written to stdout.
  source = "stdin"

  # Where to write good and bad records
  sink {
    # Sinks currently supported are:
    # 'elasticsearch' for writing good records to Elasticsearch
    # 'stdout' for writing good records to stdout
    "good": "elasticsearch"

    # Sinks currently supported are:
    # 'kinesis' for writing bad records to Kinesis
    # 'stderr' for writing bad records to stderr
    # 'none' for ignoring bad records
    "bad": "none"
  }

  # "good" for a stream of successfully enriched events
  # "bad" for a stream of bad events
  stream-type: "bad"

  # The following are used to authenticate for the Amazon Kinesis sink.
  #
  # If both are set to 'default', the default provider chain is used
  # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
  #
  # If both are set to 'iam', use AWS IAM Roles to provision credentials.
  #
  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  aws {
    access-key: ""
    secret-key: ""
  }

  kinesis {

    in {
      stream-name: "" # Kinesis stream name

      # LATEST: most recent data.
      # TRIM_HORIZON: oldest available data.
      # Note: This only affects the first run of this application
      # on a stream.
      initial-position: "TRIM_HORIZON"
    }

    out {
      # Stream for enriched events which are rejected by Elasticsearch
      stream-name: ""
      shards: 1
    }

    region: ""

    # "app-name" is used for a DynamoDB table to maintain stream state.
    # You can set it automatically using: "SnowplowElasticsearchSink-$\\{connector.kinesis.in.stream-name\\}"
    app-name: ""
  }

  elasticsearch {

    # Events are indexed using an Elasticsearch Client
    # - type: http or transport (will default to transport)
    # - endpoint: the cluster endpoint
    # - port: the port the cluster can be accessed on
    #   - for http this is usually 9200
    #   - for transport this is usually 9300
    # - max-timeout: the maximum attempt time before a client restart
    client {
      type: "http"
      endpoint: "localhost"
      port: 9200
      max-timeout: "10000"

      # Section for configuring the HTTP client
      http {
        conn-timeout: "10000"
        read-timeout: "10000"
      }
    }

    cluster {
      name: "elasticsearch"
      index: "bad"
      type: "bad"
    }
  }


  # Events are accumulated in a buffer before being sent to Elasticsearch.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byte-limit or
  # - the number of stored records exceeds record-limit or
  # - the time in milliseconds since it was last emptied exceeds time-limit
  buffer {
    byte-limit: 5242880
    record-limit: 10000
    time-limit: 60000
  }
}
